{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPGffixrHSFa+GnMrW2+CFo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaveyBae/Essentials-for-NLP/blob/main/clip_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLRkHr8_LPHL",
        "outputId": "64f584aa-8dbe-4e1f-8c75-8c1abdb1c9fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vwsd_experiment'...\n",
            "remote: Enumerating objects: 1620, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 1620 (delta 15), reused 19 (delta 9), pack-reused 1585 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1620/1620), 569.76 MiB | 45.08 MiB/s, done.\n",
            "Resolving deltas: 100% (375/375), done.\n",
            "Updating files: 100% (1017/1017), done.\n",
            "/content/vwsd_experiment\n",
            "Processing /content/vwsd_experiment\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from vwsd==0.0.0) (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from vwsd==0.0.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from vwsd==0.0.0) (4.57.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from vwsd==0.0.0) (11.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from vwsd==0.0.0) (2.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from vwsd==0.0.0) (0.23.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from vwsd==0.0.0) (3.10.0)\n",
            "Collecting ranx (from vwsd==0.0.0)\n",
            "  Downloading ranx-0.3.21-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (from vwsd==0.0.0) (5.1.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->vwsd==0.0.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->vwsd==0.0.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->vwsd==0.0.0) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->vwsd==0.0.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->vwsd==0.0.0) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->vwsd==0.0.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->vwsd==0.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->vwsd==0.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->vwsd==0.0.0) (2025.2)\n",
            "Requirement already satisfied: numba>=0.54.1 in /usr/local/lib/python3.12/dist-packages (from ranx->vwsd==0.0.0) (0.60.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from ranx->vwsd==0.0.0) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from ranx->vwsd==0.0.0) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ranx->vwsd==0.0.0) (1.16.2)\n",
            "Collecting ir-datasets (from ranx->vwsd==0.0.0)\n",
            "  Downloading ir_datasets-0.5.11-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from ranx->vwsd==0.0.0) (13.9.4)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.12/dist-packages (from ranx->vwsd==0.0.0) (3.11.3)\n",
            "Collecting lz4 (from ranx->vwsd==0.0.0)\n",
            "  Downloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting cbor2 (from ranx->vwsd==0.0.0)\n",
            "  Downloading cbor2-5.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from ranx->vwsd==0.0.0) (0.13.2)\n",
            "Collecting fastparquet (from ranx->vwsd==0.0.0)\n",
            "  Downloading fastparquet-2024.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->vwsd==0.0.0) (1.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->vwsd==0.0.0) (0.35.3)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->vwsd==0.0.0) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->vwsd==0.0.0) (3.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->vwsd==0.0.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->vwsd==0.0.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->vwsd==0.0.0) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->vwsd==0.0.0) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->vwsd==0.0.0) (0.6.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->vwsd==0.0.0) (1.1.10)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54.1->ranx->vwsd==0.0.0) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->vwsd==0.0.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->vwsd==0.0.0) (1.3.0)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.12/dist-packages (from fastparquet->ranx->vwsd==0.0.0) (2.11.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from ir-datasets->ranx->vwsd==0.0.0) (4.13.5)\n",
            "Collecting inscriptis>=2.2.0 (from ir-datasets->ranx->vwsd==0.0.0)\n",
            "  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.12/dist-packages (from ir-datasets->ranx->vwsd==0.0.0) (5.4.0)\n",
            "Collecting trec-car-tools>=2.5.4 (from ir-datasets->ranx->vwsd==0.0.0)\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
            "Collecting warc3-wet>=0.2.3 (from ir-datasets->ranx->vwsd==0.0.0)\n",
            "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting warc3-wet-clueweb09>=0.2.5 (from ir-datasets->ranx->vwsd==0.0.0)\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zlib-state>=0.1.3 (from ir-datasets->ranx->vwsd==0.0.0)\n",
            "  Downloading zlib_state-0.1.10-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting ijson>=3.1.3 (from ir-datasets->ranx->vwsd==0.0.0)\n",
            "  Downloading ijson-3.4.0.post0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Collecting unlzw3>=0.2.1 (from ir-datasets->ranx->vwsd==0.0.0)\n",
            "  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.12/dist-packages (from ir-datasets->ranx->vwsd==0.0.0) (18.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->vwsd==0.0.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->vwsd==0.0.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->vwsd==0.0.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->vwsd==0.0.0) (2025.10.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->vwsd==0.0.0) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->ranx->vwsd==0.0.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->ranx->vwsd==0.0.0) (2.19.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers->vwsd==0.0.0) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers->vwsd==0.0.0) (3.6.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets->ranx->vwsd==0.0.0) (2.8)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->ranx->vwsd==0.0.0) (0.1.2)\n",
            "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir-datasets->ranx->vwsd==0.0.0)\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading ranx-0.3.21-py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.3/99.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cbor2-5.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.9/284.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastparquet-2024.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ir_datasets-0.5.11-py3-none-any.whl (866 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.1/866.1 kB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ijson-3.4.0.post0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Downloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
            "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
            "Downloading zlib_state-0.1.10-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Building wheels for collected packages: vwsd, warc3-wet-clueweb09, cbor\n",
            "  Building wheel for vwsd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vwsd: filename=vwsd-0.0.0-py3-none-any.whl size=12008 sha256=fe046449a55970c9e5f4478abe0610565dca1b3407cc5df4b7c531dbde4ed534\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-51dlx1fi/wheels/c5/57/fd/2c2cf7eb3a4779154cd3b7c371f2e5f7443e15566d2b1c13b0\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18919 sha256=e446e623232dca2070bf747373d82f22268906f951535b5739b90f48fde7171a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/85/c2/9f0f621def52a1d5db7d29984f81e45f9fb6dfeb1a4eb6e31c\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp312-cp312-linux_x86_64.whl size=55024 sha256=91752b4160d3d09cfc3b453db2763b363b654f0a70635087ea9d0dded689497c\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/3e/21/a739cbcc331a1ab45c326d6edbdac6118de4402f6076e30ff1\n",
            "Successfully built vwsd warc3-wet-clueweb09 cbor\n",
            "Installing collected packages: warc3-wet-clueweb09, warc3-wet, cbor, zlib-state, unlzw3, trec-car-tools, lz4, ijson, cbor2, inscriptis, ir-datasets, fastparquet, ranx, vwsd\n",
            "Successfully installed cbor-1.0.0 cbor2-5.7.0 fastparquet-2024.11.0 ijson-3.4.0.post0 inscriptis-2.6.0 ir-datasets-0.5.11 lz4-4.4.4 ranx-0.3.21 trec-car-tools-2.6 unlzw3-0.2.3 vwsd-0.0.0 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.10\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/asahi417/vwsd_experiment\n",
        "%cd vwsd_experiment\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 英语数据集\n",
        "!vwsd-clip-baseline -l en --plot\n",
        "\n",
        "# 法语数据集\n",
        "!vwsd-clip-baseline -l fa --plot\n",
        "\n",
        "# 意大利语数据集\n",
        "!vwsd-clip-baseline -l it --plot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_NuNI2hMxo8",
        "outputId": "a2575018-c48a-46c9-98c5-95cf89eb141b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-22 18:41:11.598185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761158471.618106    1057 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761158471.624163    1057 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761158471.639121    1057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761158471.639152    1057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761158471.639157    1057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761158471.639161    1057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 18:41:11.643750: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 4.76kB [00:00, 17.5MB/s]\n",
            "pytorch_model.bin: 100% 1.71G/1.71G [00:19<00:00, 89.9MB/s]\n",
            "model.safetensors:   6% 103M/1.71G [00:03<00:46, 34.3MB/s] Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "\n",
            "preprocessor_config.json: 100% 316/316 [00:00<00:00, 1.45MB/s]\n",
            "\n",
            "tokenizer_config.json: 100% 844/844 [00:00<00:00, 6.47MB/s]\n",
            "\n",
            "vocab.json: 0.00B [00:00, ?B/s]\u001b[A\n",
            "vocab.json: 862kB [00:00, 2.20MB/s]\n",
            "\n",
            "merges.txt: 0.00B [00:00, ?B/s]\u001b[A\n",
            "merges.txt: 525kB [00:00, 1.01MB/s]\n",
            "\n",
            "tokenizer.json: 0.00B [00:00, ?B/s]\u001b[A\n",
            "tokenizer.json: 488kB [00:00, 1.36MB/s]\u001b[A\n",
            "tokenizer.json: 680kB [00:00, 1.52MB/s]\u001b[A\n",
            "tokenizer.json: 870kB [00:00, 1.63MB/s]\u001b[A\n",
            "tokenizer.json: 1.06MB [00:00, 1.65MB/s]\u001b[A\n",
            "tokenizer.json: 1.24MB [00:00, 1.67MB/s]\u001b[A\n",
            "tokenizer.json: 1.47MB [00:00, 1.70MB/s]\u001b[A\n",
            "tokenizer.json: 2.22MB [00:01, 2.01MB/s]\n",
            "\n",
            "special_tokens_map.json: 100% 389/389 [00:00<00:00, 2.71MB/s]\n",
            "model.safetensors: 100% 1.71G/1.71G [00:31<00:00, 54.0MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/vwsd/plot.py:27: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  plt.figure(figsize=(22, 14))\n",
            "^C\n",
            "2025-10-22 19:00:05.899223: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761159606.188093    5851 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761159606.269120    5851 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761159606.865781    5851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761159606.865824    5851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761159606.865829    5851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761159606.865834    5851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 19:00:06.919561: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "modules.json: 100% 122/122 [00:00<00:00, 879kB/s]\n",
            "config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 1.13MB/s]\n",
            "README.md: 1.91kB [00:00, 8.56MB/s]\n",
            "special_tokens_map.json: 100% 389/389 [00:00<00:00, 3.28MB/s]\n",
            "preprocessor_config.json: 100% 316/316 [00:00<00:00, 2.60MB/s]\n",
            "config.json: 4.03kB [00:00, 14.3MB/s]\n",
            "tokenizer_config.json: 100% 604/604 [00:00<00:00, 3.66MB/s]\n",
            "vocab.json: 0.00B [00:00, ?B/s]\n",
            "0_CLIPModel/pytorch_model.bin:   0% 0.00/605M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "vocab.json: 961kB [00:00, 24.6MB/s]\n",
            "merges.txt: 525kB [00:00, 16.3MB/s]\n",
            "\n",
            "0_CLIPModel/pytorch_model.bin:   0% 1.61M/605M [00:00<04:25, 2.28MB/s]\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:   0% 108k/605M [00:01<1:34:49, 106kB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:   0% 1.09M/605M [00:01<07:45, 1.30MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:   2% 9.35M/605M [00:01<00:44, 13.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:   3% 20.5M/605M [00:01<00:19, 30.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:   5% 29.5M/605M [00:01<00:14, 40.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:   9% 57.4M/605M [00:01<00:06, 90.5MB/s]\u001b[A\u001b[A\n",
            "0_CLIPModel/pytorch_model.bin:  11% 68.7M/605M [00:01<00:13, 40.4MB/s]\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  12% 73.1M/605M [00:02<00:09, 55.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  16% 99.0M/605M [00:02<00:06, 76.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  19% 115M/605M [00:02<00:08, 56.3MB/s] \u001b[A\u001b[A\n",
            "0_CLIPModel/pytorch_model.bin:  22% 136M/605M [00:03<00:11, 42.7MB/s] \u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  22% 134M/605M [00:03<00:13, 34.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  25% 151M/605M [00:03<00:10, 43.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  28% 167M/605M [00:04<00:08, 49.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  30% 179M/605M [00:04<00:08, 48.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  31% 186M/605M [00:04<00:08, 49.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  37% 221M/605M [00:05<00:06, 55.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  39% 235M/605M [00:05<00:08, 41.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  43% 263M/605M [00:05<00:05, 62.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  49% 295M/605M [00:06<00:04, 69.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  52% 316M/605M [00:06<00:03, 84.4MB/s]\u001b[A\u001b[A\n",
            "0_CLIPModel/pytorch_model.bin:  34% 203M/605M [00:08<00:19, 20.9MB/s]\u001b[A\n",
            "0_CLIPModel/pytorch_model.bin:  56% 337M/605M [00:08<00:05, 45.7MB/s]\u001b[A\n",
            "0_CLIPModel/pytorch_model.bin:  67% 404M/605M [00:08<00:03, 61.5MB/s]\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  55% 331M/605M [00:08<00:13, 20.2MB/s]\u001b[A\u001b[A\n",
            "0_CLIPModel/pytorch_model.bin:  78% 471M/605M [00:09<00:01, 76.8MB/s]\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  56% 341M/605M [00:09<00:12, 20.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  58% 352M/605M [00:09<00:09, 25.3MB/s]\u001b[A\u001b[A\n",
            "0_CLIPModel/pytorch_model.bin:  89% 538M/605M [00:09<00:00, 86.8MB/s]\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  63% 381M/605M [00:09<00:06, 36.7MB/s]\u001b[A\u001b[A\n",
            "0_CLIPModel/pytorch_model.bin: 100% 605M/605M [00:10<00:00, 59.6MB/s]\n",
            "\n",
            "\n",
            "0_CLIPModel/model.safetensors:  65% 394M/605M [00:10<00:05, 38.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  68% 411M/605M [00:10<00:04, 46.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  69% 419M/605M [00:10<00:03, 50.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors:  79% 476M/605M [00:10<00:01, 115MB/s] \u001b[A\u001b[A\n",
            "\n",
            "0_CLIPModel/model.safetensors: 100% 605M/605M [00:12<00:00, 47.5MB/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "modules.json: 100% 341/341 [00:00<00:00, 3.27MB/s]\n",
            "config_sentence_transformers.json: 100% 122/122 [00:00<00:00, 1.33MB/s]\n",
            "README.md: 5.65kB [00:00, 25.7MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 541kB/s]\n",
            "config.json: 100% 572/572 [00:00<00:00, 5.20MB/s]\n",
            "model.safetensors: 100% 539M/539M [00:06<00:00, 83.4MB/s]\n",
            "tokenizer_config.json: 100% 371/371 [00:00<00:00, 3.74MB/s]\n",
            "vocab.txt: 996kB [00:00, 21.7MB/s]\n",
            "tokenizer.json: 1.96MB [00:00, 104MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 791kB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 1.94MB/s]\n",
            "config.json: 100% 115/115 [00:00<00:00, 1.11MB/s]\n",
            "2_Dense/model.safetensors: 100% 1.57M/1.57M [00:00<00:00, 5.82MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/vwsd/plot.py:27: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  plt.figure(figsize=(22, 14))\n",
            "2025-10-22 19:04:38.903600: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761159878.936692    7061 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761159878.946926    7061 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761159878.972120    7061 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761159878.972159    7061 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761159878.972167    7061 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761159878.972174    7061 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 19:04:38.979507: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "/usr/local/lib/python3.12/dist-packages/vwsd/plot.py:27: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  plt.figure(figsize=(22, 14))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -name \"gold.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMFM9AFqX72M",
        "outputId": "a0fea15e-feea-4730-fd15-920f7efdf588"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "find: ‘/proc/64/task/64/net’: Invalid argument\n",
            "find: ‘/proc/64/net’: Invalid argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "266962d6",
        "outputId": "50ab5320-7bf5-4719-b812-6d806cf7c3d7"
      },
      "source": [
        "!unzip /content/test.data.v1.1.gold.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/test.data.v1.1.gold.zip\n",
            "  inflating: en.test.data.v1.1.txt   \n",
            "  inflating: en.test.gold.v1.1.txt   \n",
            "  inflating: fa.test.data.txt        \n",
            "  inflating: fa.test.gold.txt        \n",
            "  inflating: it.test.data.v1.1.txt   \n",
            "  inflating: it.test.gold.v1.1.txt   \n",
            "  inflating: README.txt              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 英文 (en) 文件：图像名 + \"en\"\n",
        "!awk '{print $0 \"\\t\" \"en\"}' en.test.gold.v1.1.txt > en.gold.tmp\n",
        "\n",
        "# 2. 波斯语 (fa) 文件：图像名 + \"fa\"\n",
        "!!awk '{print $0 \"\\t\" \"fa\"}' fa.test.gold.txt > fa.gold.tmp\n",
        "\n",
        "# 3. 意大利语 (it) 文件：图像名 + \"it\"\n",
        "!awk '{print $0 \"\\t\" \"it\"}' it.test.gold.v1.1.txt > it.gold.tmp\n",
        "\n",
        "!cat en.gold.tmp fa.gold.tmp it.gold.tmp > all.gold.tmp"
      ],
      "metadata": {
        "id": "gHuKtMGZdSM2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the new temporary gold file:\n",
        "!vwsd-ranking-metric -p result/Example_of_an_image_caption_that_explains_mask..target_phrase -r all.gold.tmp\n",
        "!vwsd-ranking-metric -p result/Example_of_an_image_caption_that_explains_mask..target_word -r all.gold.tmp\n",
        "!vwsd-ranking-metric -p result/mask.target_phrase -r all.gold.tmp\n",
        "!vwsd-ranking-metric -p result/mask.target_word -r all.gold.tmp\n",
        "!vwsd-ranking-metric -p result/This_is_mask..target_phrase -r all.gold.tmp\n",
        "!vwsd-ranking-metric -p result/This_is_mask..target_word -r all.gold.tmp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teZrbEccdXxO",
        "outputId": "575319ec-d1fa-4cb6-e179-b93e217a4722"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-22 19:59:02.583996: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761163142.619965   20955 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761163142.627201   20955 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761163142.644200   20955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163142.644231   20955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163142.644235   20955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163142.644240   20955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 19:59:02.649030: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "| model                                                                |   mrr_official/en |   hit_official/en |   hit_rate@1/en |   map@5/en |   mrr@5/en |   ndcg@5/en |   map@10/en |   mrr@10/en |   ndcg@10/en |   mrr_official/fa |   hit_official/fa |   hit_rate@1/fa |   map@5/fa |   mrr@5/fa |   ndcg@5/fa |   map@10/fa |   mrr@10/fa |   ndcg@10/fa |   mrr_official/it |   hit_official/it |   hit_rate@1/it |   map@5/it |   mrr@5/it |   ndcg@5/it |   map@10/it |   mrr@10/it |   ndcg@10/it |   mrr_official/avg |   hit_official/avg |   hit_rate@1/avg |   map@5/avg |   mrr@5/avg |   ndcg@5/avg |   map@10/avg |   mrr@10/avg |   ndcg@10/avg |\n",
            "|:---------------------------------------------------------------------|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|-------------------:|-------------------:|-----------------:|------------:|------------:|-------------:|-------------:|-------------:|--------------:|\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_phrase |          0.668163 |          0.50324  |        0.50324  |   0.652592 |   0.652592 |    0.71162  |    0.668163 |    0.668163 |     0.748529 |          0.38905  |             0.185 |           0.185 |   0.342583 |   0.342583 |    0.419426 |    0.38905  |    0.38905  |     0.531431 |          0.37596  |          0.190164 |        0.190164 |   0.320492 |   0.320492 |    0.382175 |    0.37596  |    0.37596  |     0.519219 |           0.477724 |           0.292801 |         0.292801 |    0.438556 |    0.438556 |     0.504407 |     0.477724 |     0.477724 |      0.599726 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_word   |          0.483614 |          0.291577 |        0.291577 |   0.44964  |   0.44964  |    0.521259 |    0.483614 |    0.483614 |     0.604849 |          0.363502 |             0.165 |           0.165 |   0.314083 |   0.314083 |    0.389235 |    0.363502 |    0.363502 |     0.510824 |          0.305109 |          0.134426 |        0.134426 |   0.236448 |   0.236448 |    0.289363 |    0.305109 |    0.305109 |     0.461987 |           0.384075 |           0.197001 |         0.197001 |    0.33339  |    0.33339  |     0.399953 |     0.384075 |     0.384075 |      0.525887 |\n",
            "| result/mask.target_phrase                                            |          0.738763 |          0.604752 |        0.604752 |   0.728582 |   0.728582 |    0.777656 |    0.738763 |    0.738763 |     0.802202 |          0.466974 |             0.285 |           0.285 |   0.431917 |   0.431917 |    0.505394 |    0.466974 |    0.466974 |     0.591721 |          0.426063 |          0.22623  |        0.22623  |   0.384809 |   0.384809 |    0.457448 |    0.426063 |    0.426063 |     0.559699 |           0.543933 |           0.371994 |         0.371994 |    0.515102 |    0.515102 |     0.580166 |     0.543933 |     0.543933 |      0.651207 |\n",
            "| result/mask.target_word                                              |          0.544272 |          0.354212 |        0.354212 |   0.518862 |   0.518862 |    0.588232 |    0.544272 |    0.544272 |     0.652186 |          0.389635 |             0.205 |           0.205 |   0.345833 |   0.345833 |    0.421285 |    0.389635 |    0.389635 |     0.530641 |          0.292006 |          0.114754 |        0.114754 |   0.22541  |   0.22541  |    0.281829 |    0.292006 |    0.292006 |     0.451744 |           0.408638 |           0.224655 |         0.224655 |    0.363369 |    0.363369 |     0.430449 |     0.408638 |     0.408638 |      0.544857 |\n",
            "| result/This_is_mask..target_phrase                                   |          0.746562 |          0.613391 |        0.613391 |   0.737221 |   0.737221 |    0.786229 |    0.746562 |    0.746562 |     0.808288 |          0.431677 |             0.23  |           0.23  |   0.39675  |   0.39675  |    0.478827 |    0.431677 |    0.431677 |     0.565028 |          0.44585  |          0.236066 |        0.236066 |   0.40694  |   0.40694  |    0.481727 |    0.44585  |    0.44585  |     0.576038 |           0.541363 |           0.359819 |         0.359819 |    0.513637 |    0.513637 |     0.582261 |     0.541363 |     0.541363 |      0.649784 |\n",
            "| result/This_is_mask..target_word                                     |          0.534535 |          0.343413 |        0.343413 |   0.507883 |   0.507883 |    0.57969  |    0.534535 |    0.534535 |     0.644897 |          0.38375  |             0.195 |           0.195 |   0.337167 |   0.337167 |    0.410074 |    0.38375  |    0.38375  |     0.52592  |          0.347077 |          0.15082  |        0.15082  |   0.285137 |   0.285137 |    0.344631 |    0.347077 |    0.347077 |     0.496778 |           0.421787 |           0.229744 |         0.229744 |    0.376729 |    0.376729 |     0.444798 |     0.421787 |     0.421787 |      0.555865 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_phrase |          0.668163 |          0.50324  |        0.50324  |   0.652592 |   0.652592 |    0.71162  |    0.668163 |    0.668163 |     0.748529 |          0.38905  |             0.185 |           0.185 |   0.342583 |   0.342583 |    0.419426 |    0.38905  |    0.38905  |     0.531431 |          0.37596  |          0.190164 |        0.190164 |   0.320492 |   0.320492 |    0.382175 |    0.37596  |    0.37596  |     0.519219 |           0.477724 |           0.292801 |         0.292801 |    0.438556 |    0.438556 |     0.504407 |     0.477724 |     0.477724 |      0.599726 |\n",
            "2025-10-22 19:59:20.599002: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761163160.618562   21038 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761163160.624371   21038 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761163160.639327   21038 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163160.639351   21038 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163160.639355   21038 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163160.639358   21038 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 19:59:20.643701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "| model                                                                |   mrr_official/en |   hit_official/en |   hit_rate@1/en |   map@5/en |   mrr@5/en |   ndcg@5/en |   map@10/en |   mrr@10/en |   ndcg@10/en |   mrr_official/fa |   hit_official/fa |   hit_rate@1/fa |   map@5/fa |   mrr@5/fa |   ndcg@5/fa |   map@10/fa |   mrr@10/fa |   ndcg@10/fa |   mrr_official/it |   hit_official/it |   hit_rate@1/it |   map@5/it |   mrr@5/it |   ndcg@5/it |   map@10/it |   mrr@10/it |   ndcg@10/it |   mrr_official/avg |   hit_official/avg |   hit_rate@1/avg |   map@5/avg |   mrr@5/avg |   ndcg@5/avg |   map@10/avg |   mrr@10/avg |   ndcg@10/avg |\n",
            "|:---------------------------------------------------------------------|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|-------------------:|-------------------:|-----------------:|------------:|------------:|-------------:|-------------:|-------------:|--------------:|\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_phrase |          0.668163 |          0.50324  |        0.50324  |   0.652592 |   0.652592 |    0.71162  |    0.668163 |    0.668163 |     0.748529 |          0.38905  |             0.185 |           0.185 |   0.342583 |   0.342583 |    0.419426 |    0.38905  |    0.38905  |     0.531431 |          0.37596  |          0.190164 |        0.190164 |   0.320492 |   0.320492 |    0.382175 |    0.37596  |    0.37596  |     0.519219 |           0.477724 |           0.292801 |         0.292801 |    0.438556 |    0.438556 |     0.504407 |     0.477724 |     0.477724 |      0.599726 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_word   |          0.483614 |          0.291577 |        0.291577 |   0.44964  |   0.44964  |    0.521259 |    0.483614 |    0.483614 |     0.604849 |          0.363502 |             0.165 |           0.165 |   0.314083 |   0.314083 |    0.389235 |    0.363502 |    0.363502 |     0.510824 |          0.305109 |          0.134426 |        0.134426 |   0.236448 |   0.236448 |    0.289363 |    0.305109 |    0.305109 |     0.461987 |           0.384075 |           0.197001 |         0.197001 |    0.33339  |    0.33339  |     0.399953 |     0.384075 |     0.384075 |      0.525887 |\n",
            "| result/mask.target_phrase                                            |          0.738763 |          0.604752 |        0.604752 |   0.728582 |   0.728582 |    0.777656 |    0.738763 |    0.738763 |     0.802202 |          0.466974 |             0.285 |           0.285 |   0.431917 |   0.431917 |    0.505394 |    0.466974 |    0.466974 |     0.591721 |          0.426063 |          0.22623  |        0.22623  |   0.384809 |   0.384809 |    0.457448 |    0.426063 |    0.426063 |     0.559699 |           0.543933 |           0.371994 |         0.371994 |    0.515102 |    0.515102 |     0.580166 |     0.543933 |     0.543933 |      0.651207 |\n",
            "| result/mask.target_word                                              |          0.544272 |          0.354212 |        0.354212 |   0.518862 |   0.518862 |    0.588232 |    0.544272 |    0.544272 |     0.652186 |          0.389635 |             0.205 |           0.205 |   0.345833 |   0.345833 |    0.421285 |    0.389635 |    0.389635 |     0.530641 |          0.292006 |          0.114754 |        0.114754 |   0.22541  |   0.22541  |    0.281829 |    0.292006 |    0.292006 |     0.451744 |           0.408638 |           0.224655 |         0.224655 |    0.363369 |    0.363369 |     0.430449 |     0.408638 |     0.408638 |      0.544857 |\n",
            "| result/This_is_mask..target_phrase                                   |          0.746562 |          0.613391 |        0.613391 |   0.737221 |   0.737221 |    0.786229 |    0.746562 |    0.746562 |     0.808288 |          0.431677 |             0.23  |           0.23  |   0.39675  |   0.39675  |    0.478827 |    0.431677 |    0.431677 |     0.565028 |          0.44585  |          0.236066 |        0.236066 |   0.40694  |   0.40694  |    0.481727 |    0.44585  |    0.44585  |     0.576038 |           0.541363 |           0.359819 |         0.359819 |    0.513637 |    0.513637 |     0.582261 |     0.541363 |     0.541363 |      0.649784 |\n",
            "| result/This_is_mask..target_word                                     |          0.534535 |          0.343413 |        0.343413 |   0.507883 |   0.507883 |    0.57969  |    0.534535 |    0.534535 |     0.644897 |          0.38375  |             0.195 |           0.195 |   0.337167 |   0.337167 |    0.410074 |    0.38375  |    0.38375  |     0.52592  |          0.347077 |          0.15082  |        0.15082  |   0.285137 |   0.285137 |    0.344631 |    0.347077 |    0.347077 |     0.496778 |           0.421787 |           0.229744 |         0.229744 |    0.376729 |    0.376729 |     0.444798 |     0.421787 |     0.421787 |      0.555865 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_phrase |          0.668163 |          0.50324  |        0.50324  |   0.652592 |   0.652592 |    0.71162  |    0.668163 |    0.668163 |     0.748529 |          0.38905  |             0.185 |           0.185 |   0.342583 |   0.342583 |    0.419426 |    0.38905  |    0.38905  |     0.531431 |          0.37596  |          0.190164 |        0.190164 |   0.320492 |   0.320492 |    0.382175 |    0.37596  |    0.37596  |     0.519219 |           0.477724 |           0.292801 |         0.292801 |    0.438556 |    0.438556 |     0.504407 |     0.477724 |     0.477724 |      0.599726 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_word   |          0.483614 |          0.291577 |        0.291577 |   0.44964  |   0.44964  |    0.521259 |    0.483614 |    0.483614 |     0.604849 |          0.363502 |             0.165 |           0.165 |   0.314083 |   0.314083 |    0.389235 |    0.363502 |    0.363502 |     0.510824 |          0.305109 |          0.134426 |        0.134426 |   0.236448 |   0.236448 |    0.289363 |    0.305109 |    0.305109 |     0.461987 |           0.384075 |           0.197001 |         0.197001 |    0.33339  |    0.33339  |     0.399953 |     0.384075 |     0.384075 |      0.525887 |\n",
            "2025-10-22 19:59:37.296846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761163177.317024   21123 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761163177.323002   21123 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761163177.338521   21123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163177.338546   21123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163177.338550   21123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163177.338556   21123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 19:59:37.343234: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "| model                                                                |   mrr_official/en |   hit_official/en |   hit_rate@1/en |   map@5/en |   mrr@5/en |   ndcg@5/en |   map@10/en |   mrr@10/en |   ndcg@10/en |   mrr_official/fa |   hit_official/fa |   hit_rate@1/fa |   map@5/fa |   mrr@5/fa |   ndcg@5/fa |   map@10/fa |   mrr@10/fa |   ndcg@10/fa |   mrr_official/it |   hit_official/it |   hit_rate@1/it |   map@5/it |   mrr@5/it |   ndcg@5/it |   map@10/it |   mrr@10/it |   ndcg@10/it |   mrr_official/avg |   hit_official/avg |   hit_rate@1/avg |   map@5/avg |   mrr@5/avg |   ndcg@5/avg |   map@10/avg |   mrr@10/avg |   ndcg@10/avg |\n",
            "|:---------------------------------------------------------------------|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|-------------------:|-------------------:|-----------------:|------------:|------------:|-------------:|-------------:|-------------:|--------------:|\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_phrase |          0.668163 |          0.50324  |        0.50324  |   0.652592 |   0.652592 |    0.71162  |    0.668163 |    0.668163 |     0.748529 |          0.38905  |             0.185 |           0.185 |   0.342583 |   0.342583 |    0.419426 |    0.38905  |    0.38905  |     0.531431 |          0.37596  |          0.190164 |        0.190164 |   0.320492 |   0.320492 |    0.382175 |    0.37596  |    0.37596  |     0.519219 |           0.477724 |           0.292801 |         0.292801 |    0.438556 |    0.438556 |     0.504407 |     0.477724 |     0.477724 |      0.599726 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_word   |          0.483614 |          0.291577 |        0.291577 |   0.44964  |   0.44964  |    0.521259 |    0.483614 |    0.483614 |     0.604849 |          0.363502 |             0.165 |           0.165 |   0.314083 |   0.314083 |    0.389235 |    0.363502 |    0.363502 |     0.510824 |          0.305109 |          0.134426 |        0.134426 |   0.236448 |   0.236448 |    0.289363 |    0.305109 |    0.305109 |     0.461987 |           0.384075 |           0.197001 |         0.197001 |    0.33339  |    0.33339  |     0.399953 |     0.384075 |     0.384075 |      0.525887 |\n",
            "| result/mask.target_phrase                                            |          0.738763 |          0.604752 |        0.604752 |   0.728582 |   0.728582 |    0.777656 |    0.738763 |    0.738763 |     0.802202 |          0.466974 |             0.285 |           0.285 |   0.431917 |   0.431917 |    0.505394 |    0.466974 |    0.466974 |     0.591721 |          0.426063 |          0.22623  |        0.22623  |   0.384809 |   0.384809 |    0.457448 |    0.426063 |    0.426063 |     0.559699 |           0.543933 |           0.371994 |         0.371994 |    0.515102 |    0.515102 |     0.580166 |     0.543933 |     0.543933 |      0.651207 |\n",
            "| result/mask.target_word                                              |          0.544272 |          0.354212 |        0.354212 |   0.518862 |   0.518862 |    0.588232 |    0.544272 |    0.544272 |     0.652186 |          0.389635 |             0.205 |           0.205 |   0.345833 |   0.345833 |    0.421285 |    0.389635 |    0.389635 |     0.530641 |          0.292006 |          0.114754 |        0.114754 |   0.22541  |   0.22541  |    0.281829 |    0.292006 |    0.292006 |     0.451744 |           0.408638 |           0.224655 |         0.224655 |    0.363369 |    0.363369 |     0.430449 |     0.408638 |     0.408638 |      0.544857 |\n",
            "| result/This_is_mask..target_phrase                                   |          0.746562 |          0.613391 |        0.613391 |   0.737221 |   0.737221 |    0.786229 |    0.746562 |    0.746562 |     0.808288 |          0.431677 |             0.23  |           0.23  |   0.39675  |   0.39675  |    0.478827 |    0.431677 |    0.431677 |     0.565028 |          0.44585  |          0.236066 |        0.236066 |   0.40694  |   0.40694  |    0.481727 |    0.44585  |    0.44585  |     0.576038 |           0.541363 |           0.359819 |         0.359819 |    0.513637 |    0.513637 |     0.582261 |     0.541363 |     0.541363 |      0.649784 |\n",
            "| result/This_is_mask..target_word                                     |          0.534535 |          0.343413 |        0.343413 |   0.507883 |   0.507883 |    0.57969  |    0.534535 |    0.534535 |     0.644897 |          0.38375  |             0.195 |           0.195 |   0.337167 |   0.337167 |    0.410074 |    0.38375  |    0.38375  |     0.52592  |          0.347077 |          0.15082  |        0.15082  |   0.285137 |   0.285137 |    0.344631 |    0.347077 |    0.347077 |     0.496778 |           0.421787 |           0.229744 |         0.229744 |    0.376729 |    0.376729 |     0.444798 |     0.421787 |     0.421787 |      0.555865 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_phrase |          0.668163 |          0.50324  |        0.50324  |   0.652592 |   0.652592 |    0.71162  |    0.668163 |    0.668163 |     0.748529 |          0.38905  |             0.185 |           0.185 |   0.342583 |   0.342583 |    0.419426 |    0.38905  |    0.38905  |     0.531431 |          0.37596  |          0.190164 |        0.190164 |   0.320492 |   0.320492 |    0.382175 |    0.37596  |    0.37596  |     0.519219 |           0.477724 |           0.292801 |         0.292801 |    0.438556 |    0.438556 |     0.504407 |     0.477724 |     0.477724 |      0.599726 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_word   |          0.483614 |          0.291577 |        0.291577 |   0.44964  |   0.44964  |    0.521259 |    0.483614 |    0.483614 |     0.604849 |          0.363502 |             0.165 |           0.165 |   0.314083 |   0.314083 |    0.389235 |    0.363502 |    0.363502 |     0.510824 |          0.305109 |          0.134426 |        0.134426 |   0.236448 |   0.236448 |    0.289363 |    0.305109 |    0.305109 |     0.461987 |           0.384075 |           0.197001 |         0.197001 |    0.33339  |    0.33339  |     0.399953 |     0.384075 |     0.384075 |      0.525887 |\n",
            "| result/mask.target_phrase                                            |          0.738763 |          0.604752 |        0.604752 |   0.728582 |   0.728582 |    0.777656 |    0.738763 |    0.738763 |     0.802202 |          0.466974 |             0.285 |           0.285 |   0.431917 |   0.431917 |    0.505394 |    0.466974 |    0.466974 |     0.591721 |          0.426063 |          0.22623  |        0.22623  |   0.384809 |   0.384809 |    0.457448 |    0.426063 |    0.426063 |     0.559699 |           0.543933 |           0.371994 |         0.371994 |    0.515102 |    0.515102 |     0.580166 |     0.543933 |     0.543933 |      0.651207 |\n",
            "2025-10-22 19:59:54.011758: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761163194.032168   21206 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761163194.038143   21206 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761163194.053552   21206 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163194.053582   21206 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163194.053586   21206 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163194.053590   21206 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 19:59:54.058106: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "| model                                                                |   mrr_official/en |   hit_official/en |   hit_rate@1/en |   map@5/en |   mrr@5/en |   ndcg@5/en |   map@10/en |   mrr@10/en |   ndcg@10/en |   mrr_official/fa |   hit_official/fa |   hit_rate@1/fa |   map@5/fa |   mrr@5/fa |   ndcg@5/fa |   map@10/fa |   mrr@10/fa |   ndcg@10/fa |   mrr_official/it |   hit_official/it |   hit_rate@1/it |   map@5/it |   mrr@5/it |   ndcg@5/it |   map@10/it |   mrr@10/it |   ndcg@10/it |   mrr_official/avg |   hit_official/avg |   hit_rate@1/avg |   map@5/avg |   mrr@5/avg |   ndcg@5/avg |   map@10/avg |   mrr@10/avg |   ndcg@10/avg |\n",
            "|:---------------------------------------------------------------------|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|-------------------:|-------------------:|-----------------:|------------:|------------:|-------------:|-------------:|-------------:|--------------:|\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_phrase |          0.668163 |          0.50324  |        0.50324  |   0.652592 |   0.652592 |    0.71162  |    0.668163 |    0.668163 |     0.748529 |          0.38905  |             0.185 |           0.185 |   0.342583 |   0.342583 |    0.419426 |    0.38905  |    0.38905  |     0.531431 |          0.37596  |          0.190164 |        0.190164 |   0.320492 |   0.320492 |    0.382175 |    0.37596  |    0.37596  |     0.519219 |           0.477724 |           0.292801 |         0.292801 |    0.438556 |    0.438556 |     0.504407 |     0.477724 |     0.477724 |      0.599726 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_word   |          0.483614 |          0.291577 |        0.291577 |   0.44964  |   0.44964  |    0.521259 |    0.483614 |    0.483614 |     0.604849 |          0.363502 |             0.165 |           0.165 |   0.314083 |   0.314083 |    0.389235 |    0.363502 |    0.363502 |     0.510824 |          0.305109 |          0.134426 |        0.134426 |   0.236448 |   0.236448 |    0.289363 |    0.305109 |    0.305109 |     0.461987 |           0.384075 |           0.197001 |         0.197001 |    0.33339  |    0.33339  |     0.399953 |     0.384075 |     0.384075 |      0.525887 |\n",
            "| result/mask.target_phrase                                            |          0.738763 |          0.604752 |        0.604752 |   0.728582 |   0.728582 |    0.777656 |    0.738763 |    0.738763 |     0.802202 |          0.466974 |             0.285 |           0.285 |   0.431917 |   0.431917 |    0.505394 |    0.466974 |    0.466974 |     0.591721 |          0.426063 |          0.22623  |        0.22623  |   0.384809 |   0.384809 |    0.457448 |    0.426063 |    0.426063 |     0.559699 |           0.543933 |           0.371994 |         0.371994 |    0.515102 |    0.515102 |     0.580166 |     0.543933 |     0.543933 |      0.651207 |\n",
            "| result/mask.target_word                                              |          0.544272 |          0.354212 |        0.354212 |   0.518862 |   0.518862 |    0.588232 |    0.544272 |    0.544272 |     0.652186 |          0.389635 |             0.205 |           0.205 |   0.345833 |   0.345833 |    0.421285 |    0.389635 |    0.389635 |     0.530641 |          0.292006 |          0.114754 |        0.114754 |   0.22541  |   0.22541  |    0.281829 |    0.292006 |    0.292006 |     0.451744 |           0.408638 |           0.224655 |         0.224655 |    0.363369 |    0.363369 |     0.430449 |     0.408638 |     0.408638 |      0.544857 |\n",
            "| result/This_is_mask..target_phrase                                   |          0.746562 |          0.613391 |        0.613391 |   0.737221 |   0.737221 |    0.786229 |    0.746562 |    0.746562 |     0.808288 |          0.431677 |             0.23  |           0.23  |   0.39675  |   0.39675  |    0.478827 |    0.431677 |    0.431677 |     0.565028 |          0.44585  |          0.236066 |        0.236066 |   0.40694  |   0.40694  |    0.481727 |    0.44585  |    0.44585  |     0.576038 |           0.541363 |           0.359819 |         0.359819 |    0.513637 |    0.513637 |     0.582261 |     0.541363 |     0.541363 |      0.649784 |\n",
            "| result/This_is_mask..target_word                                     |          0.534535 |          0.343413 |        0.343413 |   0.507883 |   0.507883 |    0.57969  |    0.534535 |    0.534535 |     0.644897 |          0.38375  |             0.195 |           0.195 |   0.337167 |   0.337167 |    0.410074 |    0.38375  |    0.38375  |     0.52592  |          0.347077 |          0.15082  |        0.15082  |   0.285137 |   0.285137 |    0.344631 |    0.347077 |    0.347077 |     0.496778 |           0.421787 |           0.229744 |         0.229744 |    0.376729 |    0.376729 |     0.444798 |     0.421787 |     0.421787 |      0.555865 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_phrase |          0.668163 |          0.50324  |        0.50324  |   0.652592 |   0.652592 |    0.71162  |    0.668163 |    0.668163 |     0.748529 |          0.38905  |             0.185 |           0.185 |   0.342583 |   0.342583 |    0.419426 |    0.38905  |    0.38905  |     0.531431 |          0.37596  |          0.190164 |        0.190164 |   0.320492 |   0.320492 |    0.382175 |    0.37596  |    0.37596  |     0.519219 |           0.477724 |           0.292801 |         0.292801 |    0.438556 |    0.438556 |     0.504407 |     0.477724 |     0.477724 |      0.599726 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_word   |          0.483614 |          0.291577 |        0.291577 |   0.44964  |   0.44964  |    0.521259 |    0.483614 |    0.483614 |     0.604849 |          0.363502 |             0.165 |           0.165 |   0.314083 |   0.314083 |    0.389235 |    0.363502 |    0.363502 |     0.510824 |          0.305109 |          0.134426 |        0.134426 |   0.236448 |   0.236448 |    0.289363 |    0.305109 |    0.305109 |     0.461987 |           0.384075 |           0.197001 |         0.197001 |    0.33339  |    0.33339  |     0.399953 |     0.384075 |     0.384075 |      0.525887 |\n",
            "| result/mask.target_phrase                                            |          0.738763 |          0.604752 |        0.604752 |   0.728582 |   0.728582 |    0.777656 |    0.738763 |    0.738763 |     0.802202 |          0.466974 |             0.285 |           0.285 |   0.431917 |   0.431917 |    0.505394 |    0.466974 |    0.466974 |     0.591721 |          0.426063 |          0.22623  |        0.22623  |   0.384809 |   0.384809 |    0.457448 |    0.426063 |    0.426063 |     0.559699 |           0.543933 |           0.371994 |         0.371994 |    0.515102 |    0.515102 |     0.580166 |     0.543933 |     0.543933 |      0.651207 |\n",
            "| result/mask.target_word                                              |          0.544272 |          0.354212 |        0.354212 |   0.518862 |   0.518862 |    0.588232 |    0.544272 |    0.544272 |     0.652186 |          0.389635 |             0.205 |           0.205 |   0.345833 |   0.345833 |    0.421285 |    0.389635 |    0.389635 |     0.530641 |          0.292006 |          0.114754 |        0.114754 |   0.22541  |   0.22541  |    0.281829 |    0.292006 |    0.292006 |     0.451744 |           0.408638 |           0.224655 |         0.224655 |    0.363369 |    0.363369 |     0.430449 |     0.408638 |     0.408638 |      0.544857 |\n",
            "2025-10-22 20:00:11.714375: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761163211.736067   21289 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761163211.742285   21289 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761163211.757419   21289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163211.757464   21289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163211.757469   21289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163211.757474   21289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 20:00:11.762081: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "| model                                                                |   mrr_official/en |   hit_official/en |   hit_rate@1/en |   map@5/en |   mrr@5/en |   ndcg@5/en |   map@10/en |   mrr@10/en |   ndcg@10/en |   mrr_official/fa |   hit_official/fa |   hit_rate@1/fa |   map@5/fa |   mrr@5/fa |   ndcg@5/fa |   map@10/fa |   mrr@10/fa |   ndcg@10/fa |   mrr_official/it |   hit_official/it |   hit_rate@1/it |   map@5/it |   mrr@5/it |   ndcg@5/it |   map@10/it |   mrr@10/it |   ndcg@10/it |   mrr_official/avg |   hit_official/avg |   hit_rate@1/avg |   map@5/avg |   mrr@5/avg |   ndcg@5/avg |   map@10/avg |   mrr@10/avg |   ndcg@10/avg |\n",
            "|:---------------------------------------------------------------------|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|-------------------:|-------------------:|-----------------:|------------:|------------:|-------------:|-------------:|-------------:|--------------:|\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_phrase |          0.668163 |          0.50324  |        0.50324  |   0.652592 |   0.652592 |    0.71162  |    0.668163 |    0.668163 |     0.748529 |          0.38905  |             0.185 |           0.185 |   0.342583 |   0.342583 |    0.419426 |    0.38905  |    0.38905  |     0.531431 |          0.37596  |          0.190164 |        0.190164 |   0.320492 |   0.320492 |    0.382175 |    0.37596  |    0.37596  |     0.519219 |           0.477724 |           0.292801 |         0.292801 |    0.438556 |    0.438556 |     0.504407 |     0.477724 |     0.477724 |      0.599726 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_word   |          0.483614 |          0.291577 |        0.291577 |   0.44964  |   0.44964  |    0.521259 |    0.483614 |    0.483614 |     0.604849 |          0.363502 |             0.165 |           0.165 |   0.314083 |   0.314083 |    0.389235 |    0.363502 |    0.363502 |     0.510824 |          0.305109 |          0.134426 |        0.134426 |   0.236448 |   0.236448 |    0.289363 |    0.305109 |    0.305109 |     0.461987 |           0.384075 |           0.197001 |         0.197001 |    0.33339  |    0.33339  |     0.399953 |     0.384075 |     0.384075 |      0.525887 |\n",
            "| result/mask.target_phrase                                            |          0.738763 |          0.604752 |        0.604752 |   0.728582 |   0.728582 |    0.777656 |    0.738763 |    0.738763 |     0.802202 |          0.466974 |             0.285 |           0.285 |   0.431917 |   0.431917 |    0.505394 |    0.466974 |    0.466974 |     0.591721 |          0.426063 |          0.22623  |        0.22623  |   0.384809 |   0.384809 |    0.457448 |    0.426063 |    0.426063 |     0.559699 |           0.543933 |           0.371994 |         0.371994 |    0.515102 |    0.515102 |     0.580166 |     0.543933 |     0.543933 |      0.651207 |\n",
            "| result/mask.target_word                                              |          0.544272 |          0.354212 |        0.354212 |   0.518862 |   0.518862 |    0.588232 |    0.544272 |    0.544272 |     0.652186 |          0.389635 |             0.205 |           0.205 |   0.345833 |   0.345833 |    0.421285 |    0.389635 |    0.389635 |     0.530641 |          0.292006 |          0.114754 |        0.114754 |   0.22541  |   0.22541  |    0.281829 |    0.292006 |    0.292006 |     0.451744 |           0.408638 |           0.224655 |         0.224655 |    0.363369 |    0.363369 |     0.430449 |     0.408638 |     0.408638 |      0.544857 |\n",
            "| result/This_is_mask..target_phrase                                   |          0.746562 |          0.613391 |        0.613391 |   0.737221 |   0.737221 |    0.786229 |    0.746562 |    0.746562 |     0.808288 |          0.431677 |             0.23  |           0.23  |   0.39675  |   0.39675  |    0.478827 |    0.431677 |    0.431677 |     0.565028 |          0.44585  |          0.236066 |        0.236066 |   0.40694  |   0.40694  |    0.481727 |    0.44585  |    0.44585  |     0.576038 |           0.541363 |           0.359819 |         0.359819 |    0.513637 |    0.513637 |     0.582261 |     0.541363 |     0.541363 |      0.649784 |\n",
            "| result/This_is_mask..target_word                                     |          0.534535 |          0.343413 |        0.343413 |   0.507883 |   0.507883 |    0.57969  |    0.534535 |    0.534535 |     0.644897 |          0.38375  |             0.195 |           0.195 |   0.337167 |   0.337167 |    0.410074 |    0.38375  |    0.38375  |     0.52592  |          0.347077 |          0.15082  |        0.15082  |   0.285137 |   0.285137 |    0.344631 |    0.347077 |    0.347077 |     0.496778 |           0.421787 |           0.229744 |         0.229744 |    0.376729 |    0.376729 |     0.444798 |     0.421787 |     0.421787 |      0.555865 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_phrase |          0.668163 |          0.50324  |        0.50324  |   0.652592 |   0.652592 |    0.71162  |    0.668163 |    0.668163 |     0.748529 |          0.38905  |             0.185 |           0.185 |   0.342583 |   0.342583 |    0.419426 |    0.38905  |    0.38905  |     0.531431 |          0.37596  |          0.190164 |        0.190164 |   0.320492 |   0.320492 |    0.382175 |    0.37596  |    0.37596  |     0.519219 |           0.477724 |           0.292801 |         0.292801 |    0.438556 |    0.438556 |     0.504407 |     0.477724 |     0.477724 |      0.599726 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_word   |          0.483614 |          0.291577 |        0.291577 |   0.44964  |   0.44964  |    0.521259 |    0.483614 |    0.483614 |     0.604849 |          0.363502 |             0.165 |           0.165 |   0.314083 |   0.314083 |    0.389235 |    0.363502 |    0.363502 |     0.510824 |          0.305109 |          0.134426 |        0.134426 |   0.236448 |   0.236448 |    0.289363 |    0.305109 |    0.305109 |     0.461987 |           0.384075 |           0.197001 |         0.197001 |    0.33339  |    0.33339  |     0.399953 |     0.384075 |     0.384075 |      0.525887 |\n",
            "| result/mask.target_phrase                                            |          0.738763 |          0.604752 |        0.604752 |   0.728582 |   0.728582 |    0.777656 |    0.738763 |    0.738763 |     0.802202 |          0.466974 |             0.285 |           0.285 |   0.431917 |   0.431917 |    0.505394 |    0.466974 |    0.466974 |     0.591721 |          0.426063 |          0.22623  |        0.22623  |   0.384809 |   0.384809 |    0.457448 |    0.426063 |    0.426063 |     0.559699 |           0.543933 |           0.371994 |         0.371994 |    0.515102 |    0.515102 |     0.580166 |     0.543933 |     0.543933 |      0.651207 |\n",
            "| result/mask.target_word                                              |          0.544272 |          0.354212 |        0.354212 |   0.518862 |   0.518862 |    0.588232 |    0.544272 |    0.544272 |     0.652186 |          0.389635 |             0.205 |           0.205 |   0.345833 |   0.345833 |    0.421285 |    0.389635 |    0.389635 |     0.530641 |          0.292006 |          0.114754 |        0.114754 |   0.22541  |   0.22541  |    0.281829 |    0.292006 |    0.292006 |     0.451744 |           0.408638 |           0.224655 |         0.224655 |    0.363369 |    0.363369 |     0.430449 |     0.408638 |     0.408638 |      0.544857 |\n",
            "| result/This_is_mask..target_phrase                                   |          0.746562 |          0.613391 |        0.613391 |   0.737221 |   0.737221 |    0.786229 |    0.746562 |    0.746562 |     0.808288 |          0.431677 |             0.23  |           0.23  |   0.39675  |   0.39675  |    0.478827 |    0.431677 |    0.431677 |     0.565028 |          0.44585  |          0.236066 |        0.236066 |   0.40694  |   0.40694  |    0.481727 |    0.44585  |    0.44585  |     0.576038 |           0.541363 |           0.359819 |         0.359819 |    0.513637 |    0.513637 |     0.582261 |     0.541363 |     0.541363 |      0.649784 |\n",
            "2025-10-22 20:00:28.914782: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761163228.935078   21376 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761163228.941182   21376 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761163228.956823   21376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163228.956849   21376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163228.956853   21376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163228.956858   21376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 20:00:28.961291: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "| model                                                                |   mrr_official/en |   hit_official/en |   hit_rate@1/en |   map@5/en |   mrr@5/en |   ndcg@5/en |   map@10/en |   mrr@10/en |   ndcg@10/en |   mrr_official/fa |   hit_official/fa |   hit_rate@1/fa |   map@5/fa |   mrr@5/fa |   ndcg@5/fa |   map@10/fa |   mrr@10/fa |   ndcg@10/fa |   mrr_official/it |   hit_official/it |   hit_rate@1/it |   map@5/it |   mrr@5/it |   ndcg@5/it |   map@10/it |   mrr@10/it |   ndcg@10/it |   mrr_official/avg |   hit_official/avg |   hit_rate@1/avg |   map@5/avg |   mrr@5/avg |   ndcg@5/avg |   map@10/avg |   mrr@10/avg |   ndcg@10/avg |\n",
            "|:---------------------------------------------------------------------|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|------------------:|------------------:|----------------:|-----------:|-----------:|------------:|------------:|------------:|-------------:|-------------------:|-------------------:|-----------------:|------------:|------------:|-------------:|-------------:|-------------:|--------------:|\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_phrase |          0.668163 |          0.50324  |        0.50324  |   0.652592 |   0.652592 |    0.71162  |    0.668163 |    0.668163 |     0.748529 |          0.38905  |             0.185 |           0.185 |   0.342583 |   0.342583 |    0.419426 |    0.38905  |    0.38905  |     0.531431 |          0.37596  |          0.190164 |        0.190164 |   0.320492 |   0.320492 |    0.382175 |    0.37596  |    0.37596  |     0.519219 |           0.477724 |           0.292801 |         0.292801 |    0.438556 |    0.438556 |     0.504407 |     0.477724 |     0.477724 |      0.599726 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_word   |          0.483614 |          0.291577 |        0.291577 |   0.44964  |   0.44964  |    0.521259 |    0.483614 |    0.483614 |     0.604849 |          0.363502 |             0.165 |           0.165 |   0.314083 |   0.314083 |    0.389235 |    0.363502 |    0.363502 |     0.510824 |          0.305109 |          0.134426 |        0.134426 |   0.236448 |   0.236448 |    0.289363 |    0.305109 |    0.305109 |     0.461987 |           0.384075 |           0.197001 |         0.197001 |    0.33339  |    0.33339  |     0.399953 |     0.384075 |     0.384075 |      0.525887 |\n",
            "| result/mask.target_phrase                                            |          0.738763 |          0.604752 |        0.604752 |   0.728582 |   0.728582 |    0.777656 |    0.738763 |    0.738763 |     0.802202 |          0.466974 |             0.285 |           0.285 |   0.431917 |   0.431917 |    0.505394 |    0.466974 |    0.466974 |     0.591721 |          0.426063 |          0.22623  |        0.22623  |   0.384809 |   0.384809 |    0.457448 |    0.426063 |    0.426063 |     0.559699 |           0.543933 |           0.371994 |         0.371994 |    0.515102 |    0.515102 |     0.580166 |     0.543933 |     0.543933 |      0.651207 |\n",
            "| result/mask.target_word                                              |          0.544272 |          0.354212 |        0.354212 |   0.518862 |   0.518862 |    0.588232 |    0.544272 |    0.544272 |     0.652186 |          0.389635 |             0.205 |           0.205 |   0.345833 |   0.345833 |    0.421285 |    0.389635 |    0.389635 |     0.530641 |          0.292006 |          0.114754 |        0.114754 |   0.22541  |   0.22541  |    0.281829 |    0.292006 |    0.292006 |     0.451744 |           0.408638 |           0.224655 |         0.224655 |    0.363369 |    0.363369 |     0.430449 |     0.408638 |     0.408638 |      0.544857 |\n",
            "| result/This_is_mask..target_phrase                                   |          0.746562 |          0.613391 |        0.613391 |   0.737221 |   0.737221 |    0.786229 |    0.746562 |    0.746562 |     0.808288 |          0.431677 |             0.23  |           0.23  |   0.39675  |   0.39675  |    0.478827 |    0.431677 |    0.431677 |     0.565028 |          0.44585  |          0.236066 |        0.236066 |   0.40694  |   0.40694  |    0.481727 |    0.44585  |    0.44585  |     0.576038 |           0.541363 |           0.359819 |         0.359819 |    0.513637 |    0.513637 |     0.582261 |     0.541363 |     0.541363 |      0.649784 |\n",
            "| result/This_is_mask..target_word                                     |          0.534535 |          0.343413 |        0.343413 |   0.507883 |   0.507883 |    0.57969  |    0.534535 |    0.534535 |     0.644897 |          0.38375  |             0.195 |           0.195 |   0.337167 |   0.337167 |    0.410074 |    0.38375  |    0.38375  |     0.52592  |          0.347077 |          0.15082  |        0.15082  |   0.285137 |   0.285137 |    0.344631 |    0.347077 |    0.347077 |     0.496778 |           0.421787 |           0.229744 |         0.229744 |    0.376729 |    0.376729 |     0.444798 |     0.421787 |     0.421787 |      0.555865 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_phrase |          0.668163 |          0.50324  |        0.50324  |   0.652592 |   0.652592 |    0.71162  |    0.668163 |    0.668163 |     0.748529 |          0.38905  |             0.185 |           0.185 |   0.342583 |   0.342583 |    0.419426 |    0.38905  |    0.38905  |     0.531431 |          0.37596  |          0.190164 |        0.190164 |   0.320492 |   0.320492 |    0.382175 |    0.37596  |    0.37596  |     0.519219 |           0.477724 |           0.292801 |         0.292801 |    0.438556 |    0.438556 |     0.504407 |     0.477724 |     0.477724 |      0.599726 |\n",
            "| result/Example_of_an_image_caption_that_explains_mask..target_word   |          0.483614 |          0.291577 |        0.291577 |   0.44964  |   0.44964  |    0.521259 |    0.483614 |    0.483614 |     0.604849 |          0.363502 |             0.165 |           0.165 |   0.314083 |   0.314083 |    0.389235 |    0.363502 |    0.363502 |     0.510824 |          0.305109 |          0.134426 |        0.134426 |   0.236448 |   0.236448 |    0.289363 |    0.305109 |    0.305109 |     0.461987 |           0.384075 |           0.197001 |         0.197001 |    0.33339  |    0.33339  |     0.399953 |     0.384075 |     0.384075 |      0.525887 |\n",
            "| result/mask.target_phrase                                            |          0.738763 |          0.604752 |        0.604752 |   0.728582 |   0.728582 |    0.777656 |    0.738763 |    0.738763 |     0.802202 |          0.466974 |             0.285 |           0.285 |   0.431917 |   0.431917 |    0.505394 |    0.466974 |    0.466974 |     0.591721 |          0.426063 |          0.22623  |        0.22623  |   0.384809 |   0.384809 |    0.457448 |    0.426063 |    0.426063 |     0.559699 |           0.543933 |           0.371994 |         0.371994 |    0.515102 |    0.515102 |     0.580166 |     0.543933 |     0.543933 |      0.651207 |\n",
            "| result/mask.target_word                                              |          0.544272 |          0.354212 |        0.354212 |   0.518862 |   0.518862 |    0.588232 |    0.544272 |    0.544272 |     0.652186 |          0.389635 |             0.205 |           0.205 |   0.345833 |   0.345833 |    0.421285 |    0.389635 |    0.389635 |     0.530641 |          0.292006 |          0.114754 |        0.114754 |   0.22541  |   0.22541  |    0.281829 |    0.292006 |    0.292006 |     0.451744 |           0.408638 |           0.224655 |         0.224655 |    0.363369 |    0.363369 |     0.430449 |     0.408638 |     0.408638 |      0.544857 |\n",
            "| result/This_is_mask..target_phrase                                   |          0.746562 |          0.613391 |        0.613391 |   0.737221 |   0.737221 |    0.786229 |    0.746562 |    0.746562 |     0.808288 |          0.431677 |             0.23  |           0.23  |   0.39675  |   0.39675  |    0.478827 |    0.431677 |    0.431677 |     0.565028 |          0.44585  |          0.236066 |        0.236066 |   0.40694  |   0.40694  |    0.481727 |    0.44585  |    0.44585  |     0.576038 |           0.541363 |           0.359819 |         0.359819 |    0.513637 |    0.513637 |     0.582261 |     0.541363 |     0.541363 |      0.649784 |\n",
            "| result/This_is_mask..target_word                                     |          0.534535 |          0.343413 |        0.343413 |   0.507883 |   0.507883 |    0.57969  |    0.534535 |    0.534535 |     0.644897 |          0.38375  |             0.195 |           0.195 |   0.337167 |   0.337167 |    0.410074 |    0.38375  |    0.38375  |     0.52592  |          0.347077 |          0.15082  |        0.15082  |   0.285137 |   0.285137 |    0.344631 |    0.347077 |    0.347077 |     0.496778 |           0.421787 |           0.229744 |         0.229744 |    0.376729 |    0.376729 |     0.444798 |     0.421787 |     0.421787 |      0.555865 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8QX1ToLxxHn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81c87018",
        "outputId": "e6ea716d-28bb-461e-a3bb-507ce33183bb"
      },
      "source": [
        "!vwsd-ranking-metric -p result/Example_of_an_image_caption_that_explains_mask..target_phrase -r all.gold.tmp > ranking_metrics.txt\n",
        "!vwsd-ranking-metric -p result/Example_of_an_image_caption_that_explains_mask..target_word -r all.gold.tmp >> ranking_metrics.txt\n",
        "!vwsd-ranking-metric -p result/mask.target_phrase -r all.gold.tmp >> ranking_metrics.txt\n",
        "!vwsd-ranking-metric -p result/mask.target_word -r all.gold.tmp >> ranking_metrics.txt\n",
        "!vwsd-ranking-metric -p result/This_is_mask..target_phrase -r all.gold.tmp >> ranking_metrics.txt\n",
        "!vwsd-ranking-metric -p result/This_is_mask..target_word -r all.gold.tmp >> ranking_metrics.txt"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-22 20:04:47.230955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761163487.252236   22466 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761163487.258386   22466 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761163487.274308   22466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163487.274335   22466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163487.274339   22466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163487.274342   22466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 20:04:47.279161: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-10-22 20:05:05.241846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761163505.263697   22558 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761163505.269690   22558 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761163505.284778   22558 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163505.284802   22558 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163505.284805   22558 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163505.284809   22558 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 20:05:05.289403: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-10-22 20:05:22.951561: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761163522.981174   22644 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761163522.987295   22644 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761163523.002884   22644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163523.002914   22644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163523.002918   22644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163523.002923   22644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 20:05:23.007641: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-10-22 20:05:39.606511: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761163539.626456   22728 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761163539.632622   22728 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761163539.648071   22728 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163539.648098   22728 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163539.648102   22728 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163539.648107   22728 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 20:05:39.652761: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-10-22 20:05:56.676509: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761163556.696829   22812 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761163556.702801   22812 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761163556.717943   22812 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163556.717966   22812 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163556.717969   22812 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163556.717973   22812 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 20:05:56.722288: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-10-22 20:06:14.368580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761163574.389636   22898 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761163574.400613   22898 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761163574.418739   22898 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163574.418773   22898 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163574.418777   22898 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761163574.418780   22898 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-22 20:06:14.423343: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -name \"vwsd-clip-baseline\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rHD8NaAjTVL",
        "outputId": "834d9ab6-5018-4e3f-ebc9-6ad7205848de"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/vwsd-clip-baseline\n",
            "find: ‘/proc/64/task/64/net’: Invalid argument\n",
            "find: ‘/proc/64/net’: Invalid argument\n"
          ]
        }
      ]
    }
  ]
}